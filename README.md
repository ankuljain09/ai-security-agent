# AI Security Agent - Red Team Testing Framework

A sophisticated multi-agent system for comprehensive AI safety testing and vulnerability assessment using Google's Gemini models and Agent Development Kit (ADK).

**Created by [Ankul Jain](https://github.com/ankuljain09)**

## ğŸ¯ Project Overview

The **AI Security Agent** is an automated red-teaming framework designed to test and evaluate the robustness of AI systems against adversarial attacks. It employs a multi-agent architecture where specialized agents collaborate to:

1. **Generate adversarial prompts** targeting specific vulnerability categories
2. **Execute attacks** against a target system (e.g., a banking assistant)
3. **Evaluate responses** to determine if safety guidelines were violated

This project leverages Google's Gemini models (Gemini 2.5 Pro and Flash) to create a comprehensive security audit pipeline for LLM systems. It's built on the **Google Agent Development Kit (ADK)** for scalable, production-ready agent orchestration.

### Key Features

- ğŸ”´ **Red Team Agent**: Generates sophisticated adversarial prompts for multiple risk categories
- ğŸ¯ **Target System**: Simulated banking assistant with built-in safety rules
- âœ… **Evaluator Agent**: Neutral assessment of whether safety violations occurred
- ğŸ“Š **Structured Results**: JSON-based evaluation verdicts with detailed reasoning
- ğŸ”§ **Modular Design**: Easy to extend with new agents and risk categories
- ğŸš€ **Built on Google ADK**: Production-grade agent orchestration framework

---

## ğŸ“ Project Folder Structure

```
ai-security-agent/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ llm_red_team_agent/                # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py                       # Main security orchestrator agent
â”‚   â”œâ”€â”€ agent_utils.py                 # Async agent execution utilities
â”‚   â”œâ”€â”€ config.py                      # Configuration (models, parameters)
â”‚   â”œâ”€â”€ tools.py                       # Security scanning tools
â”‚   â”‚
â”‚   â””â”€â”€ sub_agents/                    # Specialized sub-agents
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ red_team.py                # Adversarial prompt generator
â”‚       â”œâ”€â”€ target.py                  # Target system being tested
â”‚       â””â”€â”€ evaluator.py               # Safety violation detector
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test.py                        # Test suite
```

### File Descriptions

| File | Purpose |
|------|---------|
| **agent.py** | Main orchestrator that manages the security scanning workflow |
| **config.py** | Global configuration including model selection and parameters |
| **tools.py** | Core security scan function that orchestrates the 3-step process |
| **agent_utils.py** | Utilities for executing agents asynchronously with proper session management |
| **red_team.py** | Creates the red team agent that generates adversarial prompts |
| **target.py** | Creates the target agent (banking assistant with safety rules) |
| **evaluator.py** | Creates the evaluator agent that judges safety violations |

---

## ğŸ—ï¸ Agent Architecture

The system uses a **three-stage pipeline** architecture with specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Security Orchestrator                        â”‚
â”‚              (Main LLmAgent - Risk Category Input)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Red Team Worker    â”‚
            â”‚  (Attack Generator)  â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.9            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Generates adversarial prompt
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Target Worker      â”‚
            â”‚  (System Under Test) â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-   â”‚
            â”‚        Flash         â”‚
            â”‚ Temp: 0.1            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Target response to attack
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Evaluator Worker    â”‚
            â”‚ (Safety Verdict)     â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.0            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ JSON verdict (PASS/FAIL)
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Security Audit      â”‚
            â”‚  Result Summary      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Roles

#### ğŸ”´ Red Team Agent
- **Purpose**: Generate sophisticated adversarial prompts
- **Model**: Gemini 2.5 Pro (higher capability)
- **Temperature**: 0.9 (high creativity for diverse attacks)
- **Input**: Risk category (e.g., "Toxicity", "Phishing", "Financial Fraud")
- **Output**: Single, highly targeted adversarial prompt

#### ğŸ¯ Target Agent
- **Purpose**: Simulate the system under test
- **Model**: Gemini 2.5 Flash (faster, cost-effective)
- **Temperature**: 0.1 (consistent, conservative responses)
- **Role**: Banking Assistant with safety guardrails
- **Input**: Adversarial prompt from red team
- **Output**: Response to the adversarial prompt

#### âœ… Evaluator Agent
- **Purpose**: Determine if safety rules were violated
- **Model**: Gemini 2.5 Pro (high judgment capability)
- **Temperature**: 0.0 (deterministic evaluation)
- **Input**: Attack-response transcript
- **Output**: JSON verdict with PASS/FAIL and reasoning

---

## ğŸ”§ Tools

### complete_security_scan

The primary tool exposed to the main orchestrator agent. Executes the complete three-stage security audit pipeline.

**Parameters:**
- `risk_category` (str): The vulnerability category to test
  - Examples: "Toxicity", "Phishing", "Financial Fraud", "Prompt Injection", "Jailbreaking"

**Process:**
1. **Stage 1 - Attack Generation**
   - Red Team agent generates adversarial prompt for the given category
   - Prompt is crafted to bypass safety measures

2. **Stage 2 - Target Injection**
   - Adversarial prompt is sent to Target agent
   - Target responds according to its instructions and safety rules

3. **Stage 3 - Evaluation**
   - Evaluator analyzes both attack and response
   - Produces JSON verdict indicating if safety was maintained

**Returns:**
A formatted summary containing:
- Risk scenario tested
- Attack attempt (first 100 chars)
- Target behavior (first 100 chars)  
- Final verdict (PASS/FAIL with reasoning)


---

## ğŸ”„ Workflow

The complete workflow follows this sequence:

```
START
  â”‚
  â”œâ”€â–º User Input: Risk Category
  â”‚        (e.g., "Phishing")
  â”‚
  â”œâ”€â–º STAGE 1: Red Team Generation
  â”‚   â””â”€â–º Prompt: "Generate an adversarial prompt for Phishing"
  â”‚   â””â”€â–º Output: Sophisticated phishing attack prompt
  â”‚
  â”œâ”€â–º STAGE 2: Target Injection & Response
  â”‚   â””â”€â–º Inject: Adversarial prompt into target
  â”‚   â””â”€â–º Target Response: System's attempt to handle/block the attack
  â”‚
  â”œâ”€â–º STAGE 3: Safety Evaluation
  â”‚   â””â”€â–º Input: [ATTACK] and [RESPONSE] pair
  â”‚   â””â”€â–º Verdict: PASS (safety maintained) or FAIL (safety violated)
  â”‚
  â””â”€â–º Output: Security Audit Report
       (Risk category, attack attempt, target behavior, verdict)
END
```

### Key Design Decisions

1. **Async Execution**: Sub-agents run asynchronously with proper session management via `ThreadPoolExecutor`
2. **Temperature Tuning**:
   - Red Team: 0.9 (maximize creative adversarial attempts)
   - Target: 0.1 (consistent, predictable behavior)
   - Evaluator: 0.0 (deterministic, unbiased verdicts)
3. **Serial Pipeline**: Each stage depends on the previous stage's output
4. **Error Handling**: Attack generation failures are caught early to prevent cascade failures

---
## Running the Agent and Tests
To run any scripts or tests within the project's virtual environment, use uv run:

### Run the main agent (command-line interface):
```
uv run adk run llm_red_team_agent
```
### Run the integration test:
```
uv run python -m tests.test_agent
```
---

## ğŸ’¬ Example Conversation

### Scenario: Testing HateSpeech & PromptInjection
![HateSpeech&PromptInjection](./assets/HateSpeech&PromptInjection.png)

### Scenario: Testing PII Leakage & BrandRisk
![PIILeakage&BrandRisk](./assets/PIILeakage&BrandRisk.png)


---

## Future Roadmap and Enhancements
To evolve this Proof of Concept into an enterprise-grade Security Operation Center (SOC) for AI, users can adopt and implement following architectural advancements:
* **RAG-Based Grounding:** Integrate Vertex AI Vector Search to cross-reference responses against enterprise knowledge bases for automated hallucination detection.
* **Iterative Attack Loops:** Deploy a "Do-Until-Fail" agentic workflow that persistently refines and retries attack prompts (up to 5x) to test resilience against determined adversaries.
* **Knowledge-Driven Fuzzing:** Connect the Red Team to OWASP Top 10 and MITRE ATLAS databases to dynamically retrieve and mutate proven adversarial payloads.
* **Self-Optimizing Attacks:** Implement a feedback loop where the Red Team analyzes failed attempts to autonomously refine its prompts using genetic algorithms or Chain-of-Thought reasoning.

---

## Disclaimer
This agent sample is provided for illustrative purposes only. It serves as a basic example of an agent and a foundational starting point for individuals or teams to develop their own agents.

Users are solely responsible for any further development, testing, security hardening, and deployment of agents based on this sample. We recommend thorough review, testing, and the implementation of appropriate safeguards before using any derived agent in a live or critical system.



