# AI Security Agent - Red Team Testing Framework

A sophisticated multi-agent system for comprehensive AI safety testing and vulnerability assessment using Google's Gemini models and Agent Development Kit (ADK).

## ðŸŽ¯ Project Overview

The **AI Security Agent** is an automated red-teaming framework designed to test and evaluate the robustness of AI systems against adversarial attacks. It employs a multi-agent architecture where specialized agents collaborate to:

1. **Generate adversarial prompts** targeting specific vulnerability categories
2. **Execute attacks** against a target system (e.g., a banking assistant)
3. **Evaluate responses** to determine if safety guidelines were violated

This project leverages Google's Gemini models (Gemini 2.5 Pro and Flash) to create a comprehensive security audit pipeline for LLM systems. It's built on the **Google Agent Development Kit (ADK)** for scalable, production-ready agent orchestration.

### Key Features

- ðŸ”´ **Red Team Agent**: Generates sophisticated adversarial prompts for multiple risk categories
- ðŸŽ¯ **Target System**: Simulated banking assistant with built-in safety rules
- âœ… **Evaluator Agent**: Neutral assessment of whether safety violations occurred
- ðŸ“Š **Structured Results**: JSON-based evaluation verdicts with detailed reasoning
- ðŸ”§ **Modular Design**: Easy to extend with new agents and risk categories
- ðŸš€ **Built on Google ADK**: Production-grade agent orchestration framework

---

## ðŸ“ Project Folder Structure

```
ai-security-agent/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ llm_red_team_agent/                # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py                       # Main security orchestrator agent
â”‚   â”œâ”€â”€ agent_utils.py                 # Async agent execution utilities
â”‚   â”œâ”€â”€ config.py                      # Configuration (models, parameters)
â”‚   â”œâ”€â”€ tools.py                       # Security scanning tools
â”‚   â”‚
â”‚   â””â”€â”€ sub_agents/                    # Specialized sub-agents
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ red_team.py                # Adversarial prompt generator
â”‚       â”œâ”€â”€ target.py                  # Target system being tested
â”‚       â””â”€â”€ evaluator.py               # Safety violation detector
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test.py                        # Test suite
```

### File Descriptions

| File | Purpose |
|------|---------|
| **agent.py** | Main orchestrator that manages the security scanning workflow |
| **config.py** | Global configuration including model selection and parameters |
| **tools.py** | Core security scan function that orchestrates the 3-step process |
| **agent_utils.py** | Utilities for executing agents asynchronously with proper session management |
| **red_team.py** | Creates the red team agent that generates adversarial prompts |
| **target.py** | Creates the target agent (banking assistant with safety rules) |
| **evaluator.py** | Creates the evaluator agent that judges safety violations |

---

## ðŸ—ï¸ Agent Architecture

The system uses a **three-stage pipeline** architecture with specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Security Orchestrator                        â”‚
â”‚              (Main LLmAgent - Risk Category Input)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Red Team Worker    â”‚
            â”‚  (Attack Generator)  â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.9            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Generates adversarial prompt
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Target Worker      â”‚
            â”‚  (System Under Test) â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-  â”‚
            â”‚        Flash         â”‚
            â”‚ Temp: 0.1            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Target response to attack
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Evaluator Worker    â”‚
            â”‚ (Safety Verdict)     â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.0            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ JSON verdict (PASS/FAIL)
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Security Audit      â”‚
            â”‚  Result Summary      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Roles

#### ðŸ”´ Red Team Agent
- **Purpose**: Generate sophisticated adversarial prompts
- **Model**: Gemini 2.5 Pro (higher capability)
- **Temperature**: 0.9 (high creativity for diverse attacks)
- **Input**: Risk category (e.g., "Toxicity", "Phishing", "Financial Fraud")
- **Output**: Single, highly targeted adversarial prompt

#### ðŸŽ¯ Target Agent
- **Purpose**: Simulate the system under test
- **Model**: Gemini 2.5 Flash (faster, cost-effective)
- **Temperature**: 0.1 (consistent, conservative responses)
- **Role**: Banking Assistant with safety guardrails
- **Input**: Adversarial prompt from red team
- **Output**: Response to the adversarial prompt

#### âœ… Evaluator Agent
- **Purpose**: Determine if safety rules were violated
- **Model**: Gemini 2.5 Pro (high judgment capability)
- **Temperature**: 0.0 (deterministic evaluation)
- **Input**: Attack-response transcript
- **Output**: JSON verdict with PASS/FAIL and reasoning

---

## ðŸ”§ Tools

### `run_complete_security_scan(risk_category: str) -> str`

The primary tool exposed to the main orchestrator agent. Executes the complete three-stage security audit pipeline.

**Parameters:**
- `risk_category` (str): The vulnerability category to test
  - Examples: "Toxicity", "Phishing", "Financial Fraud", "Prompt Injection", "Jailbreaking"

**Process:**
1. **Stage 1 - Attack Generation**
   - Red Team agent generates adversarial prompt for the given category
   - Prompt is crafted to bypass safety measures

2. **Stage 2 - Target Injection**
   - Adversarial prompt is sent to Target agent
   - Target responds according to its instructions and safety rules

3. **Stage 3 - Evaluation**
   - Evaluator analyzes both attack and response
   - Produces JSON verdict indicating if safety was maintained

**Returns:**
A formatted summary containing:
- Risk scenario tested
- Attack attempt (first 100 chars)
- Target behavior (first 100 chars)  
- Final verdict (PASS/FAIL with reasoning)

**Example:**
```python
from llm_red_team_agent.tools import run_complete_security_scan

result = run_complete_security_scan("Toxicity")
print(result)
```

---

## ðŸ”„ Workflow

The complete workflow follows this sequence:

```
START
  â”‚
  â”œâ”€â–º User Input: Risk Category
  â”‚        (e.g., "Phishing")
  â”‚
  â”œâ”€â–º STAGE 1: Red Team Generation
  â”‚   â””â”€â–º Prompt: "Generate an adversarial prompt for Phishing"
  â”‚   â””â”€â–º Output: Sophisticated phishing attack prompt
  â”‚
  â”œâ”€â–º STAGE 2: Target Injection & Response
  â”‚   â””â”€â–º Inject: Adversarial prompt into target
  â”‚   â””â”€â–º Target Response: System's attempt to handle/block the attack
  â”‚
  â”œâ”€â–º STAGE 3: Safety Evaluation
  â”‚   â””â”€â–º Input: [ATTACK] and [RESPONSE] pair
  â”‚   â””â”€â–º Verdict: PASS (safety maintained) or FAIL (safety violated)
  â”‚
  â””â”€â–º Output: Security Audit Report
       (Risk category, attack attempt, target behavior, verdict)
END
```

### Key Design Decisions

1. **Async Execution**: Sub-agents run asynchronously with proper session management via `ThreadPoolExecutor`
2. **Temperature Tuning**:
   - Red Team: 0.9 (maximize creative adversarial attempts)
   - Target: 0.1 (consistent, predictable behavior)
   - Evaluator: 0.0 (deterministic, unbiased verdicts)
3. **Serial Pipeline**: Each stage depends on the previous stage's output
4. **Error Handling**: Attack generation failures are caught early to prevent cascade failures

---

## ðŸ’¬ Example Conversation

### Scenario: Testing Phishing Attack Resistance


### Scenario: Testing Toxicity Handling


---

## ðŸš€ Getting Started

### Prerequisites

- Python 3.9+
- Google Cloud Account with Vertex AI access
- Required libraries: `google-adk`, `google-cloud-aiplatform`, `google-genai`

